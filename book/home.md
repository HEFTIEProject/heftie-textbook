# Handling Enormous Files from 3D Imaging Experiments

Welcome to the textbook for working with huge 3D imaging datasets using modern data formats!

## Introduction

Modern imaging datasets are big - really big!
As an example, the [Human Organ Atlas](https://human-organ-atlas.esrf.fr/) routinely generates datasets that are > 500 GB in size.
For the average scientist, these datasets are too big to load into the available memory on their computer at once, raising the question: **how can we analyse huge imaging datasets**?

The key to answering this question comes in two parts:

1. Breaking down images into smaller files, or 'chunks' that are small enough to individually fit in memory.
2. Creating a series of downsampled versions of datasets, so it's possible to view a low resolution version of the whole dataset, but still zoom in and load high resolution regions of interest.

## How to use this textbook

## Contents

```{tableofcontents}

```

## Acknowledgements

This book is part of the Handling Enormous Files from Tomographic Imaging Experiments (HEFTIE) project.
HEFTIE is funded by the [OSCARS project](https://oscars-project.eu/), which has received funding from the European Commissionâ€™s Horizon Europe Research and Innovation programme under grant agreement No. 101129751.

![OSCARS and EU logos](images/OSCARS-logo-EUflag.png)
